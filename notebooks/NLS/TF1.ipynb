{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install tensorflow==2.16.1\n",
        "\n",
        "%pip install deepxde"
      ],
      "metadata": {
        "id": "Uf_8rFPWhEaH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6742a08e-b6b7-4166-8c2f-c2d35391e4be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.16.1\n",
            "  Downloading tensorflow-2.16.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (3.14.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.16.1)\n",
            "  Downloading ml_dtypes-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (25.0)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.16.1)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (1.75.1)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow==2.16.1)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.1) (3.10.0)\n",
            "Collecting numpy<2.0.0,>=1.26.0 (from tensorflow==2.16.1)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.16.1) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.1) (0.1.2)\n",
            "Downloading tensorflow-2.16.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.9/589.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, numpy, tensorboard, ml-dtypes, tensorflow\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.19.0\n",
            "    Uninstalling tensorboard-2.19.0:\n",
            "      Successfully uninstalled tensorboard-2.19.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml_dtypes 0.5.3\n",
            "    Uninstalling ml_dtypes-0.5.3:\n",
            "      Successfully uninstalled ml_dtypes-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.19.0\n",
            "    Uninstalling tensorflow-2.19.0:\n",
            "      Successfully uninstalled tensorflow-2.19.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.16.1 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tensorstore 0.1.77 requires ml_dtypes>=0.5.0, but you have ml-dtypes 0.3.2 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.16.1 which is incompatible.\n",
            "jax 0.5.3 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.3.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ml-dtypes-0.3.2 numpy-1.26.4 protobuf-4.25.8 tensorboard-2.16.2 tensorflow-2.16.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "ml_dtypes",
                  "numpy",
                  "tensorflow"
                ]
              },
              "id": "aa44321fd9c34886bc7a294ee07779e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deepxde in /usr/local/lib/python3.12/dist-packages (1.14.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from deepxde) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from deepxde) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from deepxde) (1.6.1)\n",
            "Requirement already satisfied: scikit-optimize>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from deepxde) (0.10.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from deepxde) (1.16.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize>=0.9.0->deepxde) (1.5.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize>=0.9.0->deepxde) (25.7.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize>=0.9.0->deepxde) (25.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->deepxde) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->deepxde) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->deepxde) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->deepxde) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->deepxde) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->deepxde) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->deepxde) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->deepxde) (2.9.0.post0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from pyaml>=16.9->scikit-optimize>=0.9.0->deepxde) (6.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->deepxde) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01_tJmprg0D-",
        "outputId": "22edc510-f432-4d29-9f37-f1c46ca9f8bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[91mGPU Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] \u001b[0m\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "Iteration 0, Loss Adam: 16.128950119018555\n",
            "Iteration 100, Loss Adam: 0.42226845026016235\n",
            "Iteration 200, Loss Adam: 0.36647650599479675\n",
            "Iteration 300, Loss Adam: 0.29819557070732117\n",
            "Iteration 400, Loss Adam: 0.2187994420528412\n",
            "Iteration 500, Loss Adam: 0.15566936135292053\n",
            "Iteration 600, Loss Adam: 0.12818194925785065\n",
            "Iteration 700, Loss Adam: 0.11291806399822235\n",
            "Iteration 800, Loss Adam: 0.10041521489620209\n",
            "Iteration 900, Loss Adam: 0.08914681524038315\n",
            "Iteration 1000, Loss Adam: 0.0805090069770813\n",
            "Iteration 1100, Loss Adam: 0.07425364851951599\n",
            "Iteration 1200, Loss Adam: 0.069801926612854\n",
            "Iteration 1300, Loss Adam: 0.0663832500576973\n",
            "Iteration 1400, Loss Adam: 0.06348473578691483\n",
            "Iteration 1500, Loss Adam: 0.060916826128959656\n",
            "Iteration 1600, Loss Adam: 0.05860261246562004\n",
            "Iteration 1700, Loss Adam: 0.05651606619358063\n",
            "Iteration 1800, Loss Adam: 0.054642945528030396\n",
            "Iteration 1900, Loss Adam: 0.05295870825648308\n",
            "Iteration 2000, Loss Adam: 0.05143848806619644\n",
            "Iteration 2100, Loss Adam: 0.05006678029894829\n",
            "Iteration 2200, Loss Adam: 0.048814885318279266\n",
            "Iteration 2300, Loss Adam: 0.04764093458652496\n",
            "Iteration 2400, Loss Adam: 0.04651089757680893\n",
            "Iteration 2500, Loss Adam: 0.045403532683849335\n",
            "Iteration 2600, Loss Adam: 0.04430920258164406\n",
            "Iteration 2700, Loss Adam: 0.043221279978752136\n",
            "Iteration 2800, Loss Adam: 0.04212480038404465\n",
            "Iteration 2900, Loss Adam: 0.04099837690591812\n",
            "Iteration 3000, Loss Adam: 0.03981525078415871\n",
            "Iteration 3100, Loss Adam: 0.03857485204935074\n",
            "Iteration 3200, Loss Adam: 0.03729410469532013\n",
            "Iteration 3300, Loss Adam: 0.036010049283504486\n",
            "Iteration 3400, Loss Adam: 0.03471798449754715\n",
            "Iteration 3500, Loss Adam: 0.03344354033470154\n",
            "Iteration 3600, Loss Adam: 0.033057600259780884\n",
            "Iteration 3700, Loss Adam: 0.03105059266090393\n",
            "Iteration 3800, Loss Adam: 0.029926005750894547\n",
            "Iteration 3900, Loss Adam: 0.028876369819045067\n",
            "Iteration 4000, Loss Adam: 0.02796190232038498\n",
            "Iteration 4100, Loss Adam: 0.027090370655059814\n",
            "Iteration 4200, Loss Adam: 0.026440022513270378\n",
            "Iteration 4300, Loss Adam: 0.025579363107681274\n",
            "Iteration 4400, Loss Adam: 0.02462063543498516\n",
            "Iteration 4500, Loss Adam: 0.023908156901597977\n",
            "Iteration 4600, Loss Adam: 0.02538241632282734\n",
            "Iteration 4700, Loss Adam: 0.022443149238824844\n",
            "Iteration 4800, Loss Adam: 0.021813709288835526\n",
            "Iteration 4900, Loss Adam: 0.021773027256131172\n",
            "Iteration 5000, Loss Adam: 0.020541010424494743\n",
            "Iteration 5100, Loss Adam: 0.020015673711895943\n",
            "Iteration 5200, Loss Adam: 0.0201587937772274\n",
            "Iteration 5300, Loss Adam: 0.018958615139126778\n",
            "Iteration 5400, Loss Adam: 0.018819769844412804\n",
            "Iteration 5500, Loss Adam: 0.01810292899608612\n",
            "Iteration 5600, Loss Adam: 0.01793440245091915\n",
            "Iteration 5700, Loss Adam: 0.017368901520967484\n",
            "Iteration 5800, Loss Adam: 0.017539123073220253\n",
            "Iteration 5900, Loss Adam: 0.016704419627785683\n",
            "Iteration 6000, Loss Adam: 0.01676247827708721\n",
            "Iteration 6100, Loss Adam: 0.016096290200948715\n",
            "Iteration 6200, Loss Adam: 0.021509956568479538\n",
            "Iteration 6300, Loss Adam: 0.015526204369962215\n",
            "Iteration 6400, Loss Adam: 0.015220191329717636\n",
            "Iteration 6500, Loss Adam: 0.01499844342470169\n",
            "Iteration 6600, Loss Adam: 0.014702396467328072\n",
            "Iteration 6700, Loss Adam: 0.014457643032073975\n",
            "Iteration 6800, Loss Adam: 0.014163073152303696\n",
            "Iteration 6900, Loss Adam: 0.013931136578321457\n",
            "Iteration 7000, Loss Adam: 0.01372167281806469\n",
            "Iteration 7100, Loss Adam: 0.013429015874862671\n",
            "Iteration 7200, Loss Adam: 0.014759143814444542\n",
            "Iteration 7300, Loss Adam: 0.013018406927585602\n",
            "Iteration 7400, Loss Adam: 0.01282532513141632\n",
            "Iteration 7500, Loss Adam: 0.012487917207181454\n",
            "Iteration 7600, Loss Adam: 0.012293013744056225\n",
            "Iteration 7700, Loss Adam: 0.015852956101298332\n",
            "Iteration 7800, Loss Adam: 0.011842216365039349\n",
            "Iteration 7900, Loss Adam: 0.011929880827665329\n",
            "Iteration 8000, Loss Adam: 0.011495784856379032\n",
            "Iteration 8100, Loss Adam: 0.011232898570597172\n",
            "Iteration 8200, Loss Adam: 0.011295042000710964\n",
            "Iteration 8300, Loss Adam: 0.010839506052434444\n",
            "Iteration 8400, Loss Adam: 0.010669119656085968\n",
            "Iteration 8500, Loss Adam: 0.01150632556527853\n",
            "Iteration 8600, Loss Adam: 0.010298586450517178\n",
            "Iteration 8700, Loss Adam: 0.01035451702773571\n",
            "Iteration 8800, Loss Adam: 0.011839590966701508\n",
            "Iteration 8900, Loss Adam: 0.009796243160963058\n",
            "Iteration 9000, Loss Adam: 0.00968506932258606\n",
            "Iteration 9100, Loss Adam: 0.009474493563175201\n",
            "Iteration 9200, Loss Adam: 0.009347201324999332\n",
            "Iteration 9300, Loss Adam: 0.01666838489472866\n",
            "Iteration 9400, Loss Adam: 0.009046526625752449\n",
            "Iteration 9500, Loss Adam: 0.009027124382555485\n",
            "Iteration 9600, Loss Adam: 0.00876698736101389\n",
            "Iteration 9700, Loss Adam: 0.009448335506021976\n",
            "Iteration 9800, Loss Adam: 0.012833365239202976\n",
            "Iteration 9900, Loss Adam: 0.008372737094759941\n",
            "Iteration 100: Loss L-BFGS-B = 6.19564e-03\n",
            "Iteration 200: Loss L-BFGS-B = 4.53469e-03\n",
            "Iteration 300: Loss L-BFGS-B = 3.57811e-03\n",
            "Iteration 400: Loss L-BFGS-B = 2.86033e-03\n",
            "Iteration 500: Loss L-BFGS-B = 2.26480e-03\n",
            "Iteration 600: Loss L-BFGS-B = 1.78468e-03\n",
            "Iteration 700: Loss L-BFGS-B = 1.42907e-03\n",
            "Iteration 800: Loss L-BFGS-B = 1.17079e-03\n",
            "Iteration 900: Loss L-BFGS-B = 9.82025e-04\n",
            "Iteration 1000: Loss L-BFGS-B = 8.40422e-04\n",
            "Iteration 1100: Loss L-BFGS-B = 7.32709e-04\n",
            "Iteration 1200: Loss L-BFGS-B = 6.53981e-04\n",
            "Iteration 1300: Loss L-BFGS-B = 5.71218e-04\n",
            "Iteration 1400: Loss L-BFGS-B = 4.99663e-04\n",
            "Iteration 1500: Loss L-BFGS-B = 4.44465e-04\n",
            "Iteration 1600: Loss L-BFGS-B = 3.96649e-04\n",
            "Iteration 1700: Loss L-BFGS-B = 3.46848e-04\n",
            "Iteration 1800: Loss L-BFGS-B = 3.07916e-04\n",
            "Iteration 1900: Loss L-BFGS-B = 2.77156e-04\n",
            "Iteration 2000: Loss L-BFGS-B = 2.48796e-04\n",
            "Iteration 2100: Loss L-BFGS-B = 2.25077e-04\n",
            "Iteration 2200: Loss L-BFGS-B = 2.02202e-04\n",
            "Iteration 2300: Loss L-BFGS-B = 1.82616e-04\n",
            "Iteration 2400: Loss L-BFGS-B = 1.67483e-04\n",
            "Iteration 2500: Loss L-BFGS-B = 1.51357e-04\n",
            "Iteration 2600: Loss L-BFGS-B = 1.38508e-04\n",
            "Iteration 2700: Loss L-BFGS-B = 1.28299e-04\n",
            "Iteration 2800: Loss L-BFGS-B = 1.19077e-04\n",
            "Iteration 2900: Loss L-BFGS-B = 1.12817e-04\n",
            "Iteration 3000: Loss L-BFGS-B = 1.06593e-04\n",
            "Iteration 3100: Loss L-BFGS-B = 1.02098e-04\n",
            "Iteration 3200: Loss L-BFGS-B = 9.81259e-05\n",
            "Iteration 3300: Loss L-BFGS-B = 9.28605e-05\n",
            "Iteration 3400: Loss L-BFGS-B = 8.83219e-05\n",
            "Iteration 3500: Loss L-BFGS-B = 8.26730e-05\n",
            "Iteration 3600: Loss L-BFGS-B = 7.69816e-05\n",
            "Iteration 3700: Loss L-BFGS-B = 7.26093e-05\n",
            "Iteration 3800: Loss L-BFGS-B = 6.88404e-05\n",
            "Iteration 3900: Loss L-BFGS-B = 6.53729e-05\n",
            "Iteration 4000: Loss L-BFGS-B = 6.26551e-05\n",
            "Iteration 4100: Loss L-BFGS-B = 6.06628e-05\n",
            "Iteration 4200: Loss L-BFGS-B = 5.88484e-05\n",
            "Iteration 4300: Loss L-BFGS-B = 5.68623e-05\n",
            "Iteration 4400: Loss L-BFGS-B = 5.45504e-05\n",
            "Iteration 4500: Loss L-BFGS-B = 5.21440e-05\n",
            "Iteration 4600: Loss L-BFGS-B = 4.98373e-05\n",
            "Iteration 4700: Loss L-BFGS-B = 4.73213e-05\n",
            "Iteration 4800: Loss L-BFGS-B = 4.57082e-05\n",
            "Iteration 4900: Loss L-BFGS-B = 4.41284e-05\n",
            "Iteration 5000: Loss L-BFGS-B = 4.24220e-05\n",
            "Iteration 5100: Loss L-BFGS-B = 4.09645e-05\n",
            "Iteration 5200: Loss L-BFGS-B = 3.94973e-05\n",
            "Iteration 5300: Loss L-BFGS-B = 3.84198e-05\n",
            "Iteration 5400: Loss L-BFGS-B = 3.75322e-05\n",
            "Iteration 5500: Loss L-BFGS-B = 3.65288e-05\n",
            "Iteration 5600: Loss L-BFGS-B = 3.56486e-05\n",
            "Iteration 5700: Loss L-BFGS-B = 3.47406e-05\n",
            "Iteration 5800: Loss L-BFGS-B = 3.39484e-05\n",
            "Iteration 5900: Loss L-BFGS-B = 3.32036e-05\n",
            "Iteration 6000: Loss L-BFGS-B = 3.23797e-05\n",
            "Iteration 6100: Loss L-BFGS-B = 3.12317e-05\n",
            "Iteration 6200: Loss L-BFGS-B = 3.03947e-05\n",
            "Iteration 6300: Loss L-BFGS-B = 2.95770e-05\n",
            "Iteration 6400: Loss L-BFGS-B = 2.86833e-05\n",
            "Iteration 6500: Loss L-BFGS-B = 2.79341e-05\n",
            "Iteration 6600: Loss L-BFGS-B = 2.71328e-05\n",
            "Iteration 6700: Loss L-BFGS-B = 2.64352e-05\n",
            "Iteration 6800: Loss L-BFGS-B = 2.59039e-05\n",
            "Iteration 6900: Loss L-BFGS-B = 2.53198e-05\n",
            "Iteration 7000: Loss L-BFGS-B = 2.47442e-05\n",
            "Iteration 7100: Loss L-BFGS-B = 2.42225e-05\n",
            "Iteration 7200: Loss L-BFGS-B = 2.37861e-05\n",
            "Iteration 7300: Loss L-BFGS-B = 2.33366e-05\n",
            "Iteration 7400: Loss L-BFGS-B = 2.28911e-05\n",
            "Training time: 362.2374\n",
            "Error u: 7.198571e-03\n",
            "Error v: 1.142963e-02\n",
            "Error h: 2.309609e-03\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "\n",
        "# Suppress TensorFlow logging\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Reduces TensorFlow verbosity\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Disable oneDNN optimizations if needed\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('pdf')\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from pyDOE import lhs\n",
        "\n",
        "import time\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import deepxde as dde\n",
        "import json\n",
        "\n",
        "\n",
        "# Set the random seed for NumPy\n",
        "np.random.seed(1234)\n",
        "# Set the random seed for TensorFlow\n",
        "tf.set_random_seed(1234)\n",
        "\n",
        "# Check GPU availability and print in red\n",
        "gpu_available = tf.config.list_physical_devices('GPU')\n",
        "print(\"\\033[91mGPU Available: \", gpu_available, \"\\033[0m\")\n",
        "time.sleep(2)\n",
        "\n",
        "############################\n",
        "D2=0.0\n",
        "D3=1/16\n",
        "\n",
        "R2=0.0\n",
        "R3=1/16\n",
        "############################\n",
        "\n",
        "class PhysicsInformedNN:\n",
        "    # Initialize the class\n",
        "    def __init__(self, x0, u0, v0, tb, X_f, layers, lb, ub, X_u_train):\n",
        "\n",
        "        X0 = np.concatenate((x0, 0 * x0), 1)  # (x0, 0)\n",
        "        X_lb = np.concatenate((0 * tb + lb[0], tb), 1)  # (lb[0], tb)\n",
        "        X_ub = np.concatenate((0 * tb + ub[0], tb), 1)  # (ub[0], tb)\n",
        "\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "        self.X_u_train = X_u_train\n",
        "\n",
        "        self.x0 = X0[:, 0:1]\n",
        "        self.t0 = X0[:, 1:2]\n",
        "\n",
        "        self.x_lb = X_lb[:, 0:1]\n",
        "        self.t_lb = X_lb[:, 1:2]\n",
        "\n",
        "        self.x_ub = X_ub[:, 0:1]\n",
        "        self.t_ub = X_ub[:, 1:2]\n",
        "\n",
        "        self.x_f = X_f[:, 0:1]\n",
        "        self.t_f = X_f[:, 1:2]\n",
        "\n",
        "        self.u0 = u0\n",
        "        self.v0 = v0\n",
        "\n",
        "        # Initialize NNs\n",
        "        self.layers = layers\n",
        "        self.weights, self.biases = self.initialize_NN(layers)\n",
        "\n",
        "        # tf Placeholders\n",
        "        self.x0_tf = tf.placeholder(tf.float32, shape=[None, self.x0.shape[1]])\n",
        "        self.t0_tf = tf.placeholder(tf.float32, shape=[None, self.t0.shape[1]])\n",
        "\n",
        "        self.u0_tf = tf.placeholder(tf.float32, shape=[None, self.u0.shape[1]])\n",
        "        self.v0_tf = tf.placeholder(tf.float32, shape=[None, self.v0.shape[1]])\n",
        "\n",
        "        self.x_lb_tf = tf.placeholder(tf.float32, shape=[None, self.x_lb.shape[1]])\n",
        "        self.t_lb_tf = tf.placeholder(tf.float32, shape=[None, self.t_lb.shape[1]])\n",
        "\n",
        "        self.x_ub_tf = tf.placeholder(tf.float32, shape=[None, self.x_ub.shape[1]])\n",
        "        self.t_ub_tf = tf.placeholder(tf.float32, shape=[None, self.t_ub.shape[1]])\n",
        "\n",
        "        self.x_f_tf = tf.placeholder(tf.float32, shape=[None, self.x_f.shape[1]])\n",
        "        self.t_f_tf = tf.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])\n",
        "\n",
        "        # tf Graphs\n",
        "        self.u0_pred, self.v0_pred, _, _ = self.net_uv(self.x0_tf, self.t0_tf)\n",
        "        self.u_lb_pred, self.v_lb_pred, self.u_x_lb_pred, self.v_x_lb_pred = self.net_uv(self.x_lb_tf, self.t_lb_tf)\n",
        "        self.u_ub_pred, self.v_ub_pred, self.u_x_ub_pred, self.v_x_ub_pred = self.net_uv(self.x_ub_tf, self.t_ub_tf)\n",
        "        self.f_u_pred, self.f_v_pred = self.net_f_uv(self.x_f_tf, self.t_f_tf)\n",
        "\n",
        "        # Loss\n",
        "        self.loss = tf.reduce_mean(tf.square(self.u0_tf - self.u0_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.v0_tf - self.v0_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.u_lb_pred - self.u_ub_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.v_lb_pred - self.v_ub_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.u_x_lb_pred - self.u_x_ub_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.v_x_lb_pred - self.v_x_ub_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.f_u_pred)) + \\\n",
        "                    tf.reduce_mean(tf.square(self.f_v_pred))\n",
        "\n",
        "        # Optimizers using TensorFlow 2.x\n",
        "        self.optimizer = dde.optimizers.tensorflow_compat_v1.scipy_optimizer.ScipyOptimizerInterface(\n",
        "            self.loss, method='L-BFGS-B',\n",
        "            options={'maxiter': 10000, 'maxfun': 10000, 'maxcor': 50, 'maxls': 50, 'ftol': 1.0 * np.finfo(float).eps})\n",
        "\n",
        "        #########################################################\n",
        "        self.optimizer_Adam = tf.train.AdamOptimizer()\n",
        "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
        "        self.gradients = tf.gradients(self.loss, self.weights)\n",
        "\n",
        "        # tf session\n",
        "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
        "                                                     log_device_placement=True))\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "\n",
        "        self.losses_adam = []\n",
        "        self.losses_bfgs = []\n",
        "        self.iteration_counter = 0\n",
        "\n",
        "    def initialize_NN(self, layers):\n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = tf.Variable(tf.random_normal([layers[l], layers[l + 1]], stddev=np.sqrt(1 / layers[l])),\n",
        "                            dtype=tf.float32)\n",
        "            b = tf.Variable(tf.random_normal([1, layers[l + 1]]), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)\n",
        "        return weights, biases\n",
        "\n",
        "    def neural_net(self, X, weights, biases):\n",
        "        num_layers = len(weights) + 1\n",
        "\n",
        "        H = 2.0 * (X - self.lb) / (self.ub - self.lb) - 1.0\n",
        "        for l in range(0, num_layers - 2):\n",
        "            W = weights[l]\n",
        "            b = biases[l]\n",
        "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = weights[-1]\n",
        "        b = biases[-1]\n",
        "        Y = tf.add(tf.matmul(H, W), b)\n",
        "        return Y\n",
        "\n",
        "    def net_uv(self, x, t):\n",
        "        X = tf.concat([x, t], 1)\n",
        "\n",
        "        uv = self.neural_net(X, self.weights, self.biases)\n",
        "        u = uv[:, 0:1]\n",
        "        v = uv[:, 1:2]\n",
        "\n",
        "        u_x = tf.gradients(u, x)[0]\n",
        "        v_x = tf.gradients(v, x)[0]\n",
        "\n",
        "        return u, v, u_x, v_x\n",
        "\n",
        "    def net_f_uv(self, x, t):\n",
        "        u, v, u_x, v_x = self.net_uv(x, t)\n",
        "\n",
        "        u_t = tf.gradients(u, t)[0]\n",
        "        u_xx = tf.gradients(u_x, x)[0]\n",
        "\n",
        "\n",
        "        v_t = tf.gradients(v, t)[0]\n",
        "        v_xx = tf.gradients(v_x, x)[0]\n",
        "\n",
        "\n",
        "        #   style\n",
        "        f_u = u_t + 0.5*v_xx + (u**2 + v**2)*v\n",
        "        f_v = v_t - 0.5*u_xx - (u**2 + v**2)*u\n",
        "\n",
        "\n",
        "\n",
        "        return f_u, f_v\n",
        "\n",
        "    def callback(self, loss):\n",
        "        self.iteration_counter += 1\n",
        "        if self.iteration_counter % 100 == 0:\n",
        "          print(f'Iteration {self.iteration_counter}: Loss L-BFGS-B = {loss:.5e}')\n",
        "\n",
        "    def train(self, nIter):\n",
        "\n",
        "        tf_dict = {self.x0_tf: self.x0, self.t0_tf: self.t0,\n",
        "                   self.u0_tf: self.u0, self.v0_tf: self.v0,\n",
        "                   self.x_lb_tf: self.x_lb, self.t_lb_tf: self.t_lb,\n",
        "                   self.x_ub_tf: self.x_ub, self.t_ub_tf: self.t_ub,\n",
        "                   self.x_f_tf: self.x_f, self.t_f_tf: self.t_f}\n",
        "\n",
        "        start_time = time.time()\n",
        "        for it in range(nIter):\n",
        "            _, loss_value, gradients = self.sess.run([self.train_op_Adam, self.loss, self.gradients], tf_dict)\n",
        "            self.losses_adam.append(loss_value)\n",
        "\n",
        "            # Print\n",
        "            if it % 100 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f'Iteration {it}, Loss Adam: {loss_value}')\n",
        "                start_time = time.time()\n",
        "\n",
        "        self.optimizer.minimize(self.sess, feed_dict=tf_dict, fetches=[self.loss], loss_callback=self.callback)\n",
        "\n",
        "    def predict(self, X_star):\n",
        "\n",
        "            tf_dict = {self.x0_tf: X_star[:, 0:1], self.t0_tf: X_star[:, 1:2]}\n",
        "            u_star = self.sess.run(self.u0_pred, tf_dict)\n",
        "            v_star = self.sess.run(self.v0_pred, tf_dict)\n",
        "\n",
        "            tf_dict = {self.x_f_tf: X_star[:, 0:1], self.t_f_tf: X_star[:, 1:2]}\n",
        "\n",
        "            f_u_star = self.sess.run(self.f_u_pred, tf_dict)\n",
        "            f_v_star = self.sess.run(self.f_v_pred, tf_dict)\n",
        "\n",
        "            return u_star, v_star, f_u_star, f_v_star\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    noise = 0.0\n",
        "\n",
        "    # Training domain\n",
        "    lb = np.array([-5.0, 0.0])\n",
        "    ub = np.array([ 5.0, np.pi/2])\n",
        "\n",
        "    N0 = 100\n",
        "    N_b = 100\n",
        "    N_f = 20000\n",
        "    layers = [2, 40, 40, 40, 2]\n",
        "\n",
        "    data = scipy.io.loadmat('NLS.mat')\n",
        "\n",
        "\n",
        "    t = data['tt'].flatten()[:, None]\n",
        "    x = data['x'].flatten()[:, None]\n",
        "    Exact = data['uu']\n",
        "\n",
        "    Exact_u = np.real(Exact)\n",
        "    Exact_v = np.imag(Exact)\n",
        "    Exact_h = np.sqrt(Exact_u ** 2 + Exact_v ** 2)\n",
        "\n",
        "    X, T = np.meshgrid(x, t)\n",
        "\n",
        "    X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
        "    u_star = Exact_u.T.flatten()[:, None]\n",
        "    v_star = Exact_v.T.flatten()[:, None]\n",
        "    h_star = Exact_h.T.flatten()[:, None]\n",
        "\n",
        "    idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
        "    x0 = x[idx_x, :]\n",
        "    u0 = Exact_u[idx_x, 0:1]\n",
        "    v0 = Exact_v[idx_x, 0:1]\n",
        "\n",
        "    idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
        "    tb = t[idx_t, :]\n",
        "\n",
        "    X_f = lb + (ub - lb) * lhs(2, N_f)\n",
        "\n",
        "    X0 = np.concatenate((x0, 0 * x0), 1)  # (x0, 0)\n",
        "    X_lb = np.concatenate((0 * tb + lb[0], tb), 1)  # (lb[0], tb)\n",
        "    X_ub = np.concatenate((0 * tb + ub[0], tb), 1)  # (ub[0], tb)\n",
        "    X_u_train = np.vstack([X0, X_lb, X_ub])\n",
        "\n",
        "    model = PhysicsInformedNN(x0, u0, v0, tb, X_f, layers, lb, ub, X_u_train)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.train(10000)\n",
        "\n",
        "\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "    u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_star)\n",
        "    h_pred = np.sqrt(u_pred ** 2 + v_pred ** 2)\n",
        "\n",
        "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "    error_v = np.linalg.norm(v_star - v_pred, 2) / np.linalg.norm(v_star, 2)\n",
        "    error_h = np.linalg.norm(h_star - h_pred, 2) / np.linalg.norm(h_star, 2)\n",
        "    print('Error u: %e' % (error_u))\n",
        "    print('Error v: %e' % (error_v))\n",
        "    print('Error h: %e' % (error_h))\n",
        "\n",
        "\n",
        "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
        "    V_pred = griddata(X_star, v_pred.flatten(), (X, T), method='cubic')\n",
        "    H_pred = griddata(X_star, h_pred.flatten(), (X, T), method='cubic')\n",
        "\n",
        "    U_star = griddata(X_star, u_star.flatten(), (X, T), method='cubic')\n",
        "    V_star = griddata(X_star, v_star.flatten(), (X, T), method='cubic')\n",
        "    H_star = griddata(X_star, h_star.flatten(), (X, T), method='cubic')\n",
        "\n",
        "    FU_pred = griddata(X_star, f_u_pred.flatten(), (X, T), method='cubic')\n",
        "    FV_pred = griddata(X_star, f_v_pred.flatten(), (X, T), method='cubic')\n",
        "\n",
        "\n",
        "\n",
        "########################################################################\n",
        "# Plotting the stacked time slices with prediction and exact solutions\n",
        "plt.figure(figsize=(10, 8))\n",
        "x_min, x_max = x.min(), x.max()\n",
        "cut_indices = np.linspace(0, len(t) - 1, 9, dtype=int)\n",
        "y_min, y_max = -0.1, (len(cut_indices) + 1) * 2.0\n",
        "\n",
        "for i, idx in enumerate(cut_indices):\n",
        "    vertical_offset = i * 2.0  # Adjust the spacing between lines as needed\n",
        "    plt.plot(x, Exact_h[:, idx] + vertical_offset, 'b-', linewidth=1.5, label=f'$t = {t[idx, 0]:.2f}$' if i == 0 else \"\")\n",
        "    plt.plot(x, H_pred[idx, :] + vertical_offset, 'r--', linewidth=1.5)\n",
        "    plt.text(x_min - 1, vertical_offset, f'$t = {t[idx, 0]:.2f}$', fontsize=10, verticalalignment='center')\n",
        "plt.xlabel('$x$', fontsize=14)\n",
        "plt.ylabel('')\n",
        "plt.xlim([x_min, x_max])\n",
        "plt.ylim([y_min, y_max])\n",
        "plt.gca().set_yticks([])  # Remove y-axis ticks\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.legend(['Exact', 'Prediction'], loc='upper right', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.savefig('time.pdf', dpi=300)\n",
        "\n",
        "\n",
        "############################ Plotting ###############################\n",
        "# Plotting the results\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# We do NOT transpose if the first dimension is t, second is x.\n",
        "h_img = ax.imshow(H_pred,\n",
        "                  extent=[lb[0], ub[0], lb[1], ub[1]],  # [x_min, x_max, t_min, t_max]\n",
        "                  origin='lower',\n",
        "                  aspect='auto',\n",
        "                  cmap='YlGnBu')\n",
        "\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "fig.colorbar(h_img, cax=cax, label='|Q(x, t)|')\n",
        "\n",
        "# 3) Plot training points\n",
        "ax.plot(X_u_train[:, 0], X_u_train[:, 1],\n",
        "        'kx', label='Data Points (%d)' % (X_u_train.shape[0]),\n",
        "        markersize=4, clip_on=False)\n",
        "ax.plot(X_f[:, 0], X_f[:, 1],\n",
        "        'r.', label='Collocation Points (%d)' % (X_f.shape[0]),\n",
        "        markersize=2)\n",
        "ax.legend(loc='upper right', fontsize=10, framealpha=0.8)\n",
        "ax.set_xlabel(\"x\", fontsize=14)\n",
        "ax.set_ylabel(\"t\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('data.pdf', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#############################################################################\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Plotting exact u(t, x)\n",
        "plt.subplot(3, 3, 1)\n",
        "plt.pcolor(T, X, U_star, cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Exact u(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Plotting exact v(t, x)\n",
        "plt.subplot(3, 3, 2)\n",
        "plt.pcolor(T, X, V_star, cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Exact v(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Plotting exact h(t, x)\n",
        "plt.subplot(3, 3, 3)\n",
        "plt.pcolor(T, X, H_star, cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Exact h(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Plotting predicted u(t, x)\n",
        "plt.subplot(3, 3, 4)\n",
        "plt.pcolor(T, X, U_pred, cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Predicted u(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Plotting predicted v(t, x)\n",
        "plt.subplot(3, 3, 5)\n",
        "plt.pcolor(T, X, V_pred, cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Predicted v(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Plotting predicted h(t, x)\n",
        "plt.subplot(3, 3, 6)\n",
        "plt.pcolor(T, X, H_pred, cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Predicted h(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Plotting absolute error for u(t, x)\n",
        "plt.subplot(3, 3, 7)\n",
        "plt.pcolor(T, X, np.abs(U_star - U_pred), cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Absolute error for u(t, x)')\n",
        "plt.text(0.1, 0.9, f'Error u: {error_u:.3e}', color='white', fontsize=10, transform=plt.gca().transAxes)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Plotting absolute error for v(t, x)\n",
        "plt.subplot(3, 3, 8)\n",
        "plt.pcolor(T, X, np.abs(V_star - V_pred), cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Absolute error for v(t, x)')\n",
        "plt.text(0.1, 0.9, f'Error v: {error_v:.3e}', color='white', fontsize=10, transform=plt.gca().transAxes)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Plotting absolute error for h(t, x)\n",
        "plt.subplot(3, 3, 9)\n",
        "plt.pcolor(T, X, np.abs(H_star - H_pred), cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Absolute error for h(t, x)')\n",
        "plt.text(0.1, 0.9, f'Error h: {error_h:.3e}', color='white', fontsize=10, transform=plt.gca().transAxes)\n",
        "plt.tight_layout()\n",
        "plt.savefig('U_V_H_errors.pdf', dpi=25, bbox_inches='tight')\n",
        "\n",
        "\n",
        "np.savez_compressed('PINN_results.npz',\n",
        "     x=X[0, :],                  # x-axis\n",
        "     t=T[:, 0],                  # t-axis\n",
        "     X=X,\n",
        "     T=T,\n",
        "     Exact_h = Exact_h,\n",
        "     U_star=U_star,\n",
        "     V_star=V_star,\n",
        "     H_star=H_star,\n",
        "     U_pred=U_pred,\n",
        "     V_pred=V_pred,\n",
        "     H_pred=H_pred,\n",
        "     X_u_train=X_u_train,\n",
        "     X_f=X_f,\n",
        "     lb=lb,\n",
        "     ub=ub,\n",
        "     error_u=error_u,\n",
        "     error_v=error_v,\n",
        "     error_h=error_h\n",
        ")\n",
        "\n",
        "\n",
        "metadata = {\n",
        "     'layers': layers,\n",
        "     'N0': int(N0),\n",
        "     'Nb': int(N_b),\n",
        "     'Nf': int(N_f),\n",
        "     'lb': lb.tolist(),\n",
        "     'ub': ub.tolist(),\n",
        "     'error_u': float(error_u),\n",
        "     'error_v': float(error_v),\n",
        "     'error_h': float(error_h),\n",
        "     'training_time_sec': float(elapsed)\n",
        "    }\n",
        "\n",
        "with open('PINN_metadata.json', 'w') as f:\n",
        "      json.dump(metadata, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a81c80ee",
        "outputId": "5a185aff-ccbe-4f0f-f00e-4fc1b67e04e9"
      },
      "source": [
        "!pip install pyDOE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyDOE\n",
            "  Downloading pyDOE-0.3.8.zip (22 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pyDOE) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pyDOE) (1.16.2)\n",
            "Building wheels for collected packages: pyDOE\n",
            "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyDOE: filename=pyDOE-0.3.8-py3-none-any.whl size=18170 sha256=a7f0da8d751d9f57e074121ef005d8552f3a2e4fc943af7006bfa9d69e375401\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/b9/5d/1138ea8c8f212bce6e97ae58847b7cc323145b3277f2129e2b\n",
            "Successfully built pyDOE\n",
            "Installing collected packages: pyDOE\n",
            "Successfully installed pyDOE-0.3.8\n"
          ]
        }
      ]
    }
  ]
}