{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRfbueNIyZKK",
        "outputId": "66014537-7fba-4fc4-962b-e544617d42a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyDOE\n",
            "  Downloading pyDOE-0.3.8.zip (22 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Building wheels for collected packages: pyDOE\n",
            "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyDOE: filename=pyDOE-0.3.8-py3-none-any.whl size=18170 sha256=f8a56f72aca4f767d0ea1a9ba20ef45689d9ede849c4a6dc545fd56b44fbbb71\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/b9/5d/1138ea8c8f212bce6e97ae58847b7cc323145b3277f2129e2b\n",
            "Successfully built pyDOE\n",
            "Installing collected packages: pyDOE\n",
            "Successfully installed pyDOE-0.3.8\n"
          ]
        }
      ],
      "source": [
        "!pip install pyDOE torch matplotlib scipy numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH9lXe4myn_h",
        "outputId": "b62f190d-f7bf-43e6-e149-9181d885d674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Adam Iteration 0: Loss = 2.18551e-01\n",
            "Adam Iteration 100: Loss = 1.85658e-01\n",
            "Adam Iteration 200: Loss = 1.85010e-01\n",
            "Adam Iteration 300: Loss = 1.83586e-01\n",
            "Adam Iteration 400: Loss = 1.80091e-01\n",
            "Adam Iteration 500: Loss = 1.77886e-01\n",
            "Adam Iteration 600: Loss = 1.54663e-01\n",
            "Adam Iteration 700: Loss = 1.40203e-01\n",
            "Adam Iteration 800: Loss = 1.29077e-01\n",
            "Adam Iteration 900: Loss = 1.20391e-01\n",
            "Adam Iteration 1000: Loss = 1.12479e-01\n",
            "Adam Iteration 1100: Loss = 1.04172e-01\n",
            "Adam Iteration 1200: Loss = 9.73107e-02\n",
            "Adam Iteration 1300: Loss = 9.92787e-02\n",
            "Adam Iteration 1400: Loss = 8.86948e-02\n",
            "Adam Iteration 1500: Loss = 8.55556e-02\n",
            "Adam Iteration 1600: Loss = 8.07710e-02\n",
            "Adam Iteration 1700: Loss = 7.86055e-02\n",
            "Adam Iteration 1800: Loss = 7.72135e-02\n",
            "Adam Iteration 1900: Loss = 7.63640e-02\n",
            "Adam Iteration 2000: Loss = 7.58664e-02\n",
            "Adam Iteration 2100: Loss = 7.51727e-02\n",
            "Adam Iteration 2200: Loss = 7.48060e-02\n",
            "Adam Iteration 2300: Loss = 7.45166e-02\n",
            "Adam Iteration 2400: Loss = 7.42510e-02\n",
            "Adam Iteration 2500: Loss = 7.51785e-02\n",
            "Adam Iteration 2600: Loss = 7.37971e-02\n",
            "Adam Iteration 2700: Loss = 7.35953e-02\n",
            "Adam Iteration 2800: Loss = 7.34326e-02\n",
            "Adam Iteration 2900: Loss = 7.33209e-02\n",
            "Adam Iteration 3000: Loss = 7.30691e-02\n",
            "Adam Iteration 3100: Loss = 7.32351e-02\n",
            "Adam Iteration 3200: Loss = 7.27122e-02\n",
            "Adam Iteration 3300: Loss = 7.25447e-02\n",
            "Adam Iteration 3400: Loss = 7.24137e-02\n",
            "Adam Iteration 3500: Loss = 7.26515e-02\n",
            "Adam Iteration 3600: Loss = 7.18612e-02\n",
            "Adam Iteration 3700: Loss = 7.14130e-02\n",
            "Adam Iteration 3800: Loss = 7.07181e-02\n",
            "Adam Iteration 3900: Loss = 6.92715e-02\n",
            "Adam Iteration 4000: Loss = 6.51321e-02\n",
            "Adam Iteration 4100: Loss = 5.54832e-02\n",
            "Adam Iteration 4200: Loss = 4.64374e-02\n",
            "Adam Iteration 4300: Loss = 3.40341e-02\n",
            "Adam Iteration 4400: Loss = 2.37620e-02\n",
            "Adam Iteration 4500: Loss = 1.75171e-02\n",
            "Adam Iteration 4600: Loss = 1.41206e-02\n",
            "Adam Iteration 4700: Loss = 1.21060e-02\n",
            "Adam Iteration 4800: Loss = 1.01738e-02\n",
            "Adam Iteration 4900: Loss = 1.16081e-02\n",
            "Adam Iteration 5000: Loss = 8.33628e-03\n",
            "Adam Iteration 5100: Loss = 7.74254e-03\n",
            "Adam Iteration 5200: Loss = 7.26262e-03\n",
            "Adam Iteration 5300: Loss = 6.88384e-03\n",
            "Adam Iteration 5400: Loss = 6.65006e-03\n",
            "Adam Iteration 5500: Loss = 6.36606e-03\n",
            "Adam Iteration 5600: Loss = 6.06804e-03\n",
            "Adam Iteration 5700: Loss = 6.05472e-03\n",
            "Adam Iteration 5800: Loss = 5.78098e-03\n",
            "Adam Iteration 5900: Loss = 5.46618e-03\n",
            "Adam Iteration 6000: Loss = 5.16898e-03\n",
            "Adam Iteration 6100: Loss = 5.38518e-03\n",
            "Adam Iteration 6200: Loss = 4.89946e-03\n",
            "Adam Iteration 6300: Loss = 4.87575e-03\n",
            "Adam Iteration 6400: Loss = 4.52506e-03\n",
            "Adam Iteration 6500: Loss = 4.42532e-03\n",
            "Adam Iteration 6600: Loss = 4.25630e-03\n",
            "Adam Iteration 6700: Loss = 4.66621e-03\n",
            "Adam Iteration 6800: Loss = 4.04459e-03\n",
            "Adam Iteration 6900: Loss = 3.93323e-03\n",
            "Adam Iteration 7000: Loss = 3.83914e-03\n",
            "Adam Iteration 7100: Loss = 4.63706e-03\n",
            "Adam Iteration 7200: Loss = 3.67203e-03\n",
            "Adam Iteration 7300: Loss = 4.03578e-03\n",
            "Adam Iteration 7400: Loss = 3.96727e-03\n",
            "Adam Iteration 7500: Loss = 5.08297e-03\n",
            "Adam Iteration 7600: Loss = 3.33787e-03\n",
            "Adam Iteration 7700: Loss = 6.56121e-03\n",
            "Adam Iteration 7800: Loss = 3.59661e-03\n",
            "Adam Iteration 7900: Loss = 3.22336e-03\n",
            "Adam Iteration 8000: Loss = 3.08249e-03\n",
            "Adam Iteration 8100: Loss = 3.02869e-03\n",
            "Adam Iteration 8200: Loss = 2.98357e-03\n",
            "Adam Iteration 8300: Loss = 2.93084e-03\n",
            "Adam Iteration 8400: Loss = 3.10081e-03\n",
            "Adam Iteration 8500: Loss = 3.34851e-03\n",
            "Adam Iteration 8600: Loss = 3.92697e-03\n",
            "Adam Iteration 8700: Loss = 2.95926e-03\n",
            "Adam Iteration 8800: Loss = 2.69334e-03\n",
            "Adam Iteration 8900: Loss = 2.72996e-03\n",
            "Adam Iteration 9000: Loss = 2.84184e-03\n",
            "Adam Iteration 9100: Loss = 3.56175e-03\n",
            "Adam Iteration 9200: Loss = 3.62194e-03\n",
            "Adam Iteration 9300: Loss = 4.26861e-03\n",
            "Adam Iteration 9400: Loss = 2.97228e-03\n",
            "Adam Iteration 9500: Loss = 3.73247e-03\n",
            "Adam Iteration 9600: Loss = 2.92552e-03\n",
            "Adam Iteration 9700: Loss = 2.68098e-03\n",
            "Adam Iteration 9800: Loss = 2.40161e-03\n",
            "Adam Iteration 9900: Loss = 2.35692e-03\n",
            "Adam Iteration 10000: Loss = 5.87850e-03\n",
            "Adam Iteration 10100: Loss = 2.33043e-03\n",
            "Adam Iteration 10200: Loss = 2.22614e-03\n",
            "Adam Iteration 10300: Loss = 2.20915e-03\n",
            "Adam Iteration 10400: Loss = 2.16781e-03\n",
            "Adam Iteration 10500: Loss = 2.76629e-03\n",
            "Adam Iteration 10600: Loss = 3.94985e-03\n",
            "Adam Iteration 10700: Loss = 2.09189e-03\n",
            "Adam Iteration 10800: Loss = 2.78636e-03\n",
            "Adam Iteration 10900: Loss = 2.31832e-03\n",
            "Adam Iteration 11000: Loss = 2.25985e-03\n",
            "Adam Iteration 11100: Loss = 4.75755e-03\n",
            "Adam Iteration 11200: Loss = 1.96653e-03\n",
            "Adam Iteration 11300: Loss = 2.34042e-03\n",
            "Adam Iteration 11400: Loss = 1.97089e-03\n",
            "Adam Iteration 11500: Loss = 4.48534e-03\n",
            "Adam Iteration 11600: Loss = 2.13099e-03\n",
            "Adam Iteration 11700: Loss = 1.85665e-03\n",
            "Adam Iteration 11800: Loss = 1.87917e-03\n",
            "Adam Iteration 11900: Loss = 1.80335e-03\n",
            "Adam Iteration 12000: Loss = 2.11673e-03\n",
            "Adam Iteration 12100: Loss = 1.77992e-03\n",
            "Adam Iteration 12200: Loss = 1.78542e-03\n",
            "Adam Iteration 12300: Loss = 1.74701e-03\n",
            "Adam Iteration 12400: Loss = 1.67709e-03\n",
            "Adam Iteration 12500: Loss = 1.65455e-03\n",
            "Adam Iteration 12600: Loss = 1.63908e-03\n",
            "Adam Iteration 12700: Loss = 1.63844e-03\n",
            "Adam Iteration 12800: Loss = 1.59093e-03\n",
            "Adam Iteration 12900: Loss = 1.57955e-03\n",
            "Adam Iteration 13000: Loss = 1.57788e-03\n",
            "Adam Iteration 13100: Loss = 1.63353e-03\n",
            "Adam Iteration 13200: Loss = 2.15161e-03\n",
            "Adam Iteration 13300: Loss = 3.05482e-03\n",
            "Adam Iteration 13400: Loss = 4.16990e-03\n",
            "Adam Iteration 13500: Loss = 1.59084e-03\n",
            "Adam Iteration 13600: Loss = 1.43994e-03\n",
            "Adam Iteration 13700: Loss = 1.43963e-03\n",
            "Adam Iteration 13800: Loss = 1.46902e-03\n",
            "Adam Iteration 13900: Loss = 1.38827e-03\n",
            "Adam Iteration 14000: Loss = 1.36864e-03\n",
            "Adam Iteration 14100: Loss = 1.39298e-03\n",
            "Adam Iteration 14200: Loss = 1.35367e-03\n",
            "Adam Iteration 14300: Loss = 1.31405e-03\n",
            "Adam Iteration 14400: Loss = 1.30073e-03\n",
            "Adam Iteration 14500: Loss = 5.03976e-03\n",
            "Adam Iteration 14600: Loss = 1.36655e-03\n",
            "Adam Iteration 14700: Loss = 1.32304e-03\n",
            "Adam Iteration 14800: Loss = 1.33245e-03\n",
            "Adam Iteration 14900: Loss = 1.31066e-03\n",
            "Adam Iteration 15000: Loss = 1.41482e-03\n",
            "Adam Iteration 15100: Loss = 1.27970e-03\n",
            "Adam Iteration 15200: Loss = 2.23767e-03\n",
            "Adam Iteration 15300: Loss = 1.27920e-03\n",
            "Adam Iteration 15400: Loss = 1.40575e-03\n",
            "Adam Iteration 15500: Loss = 3.20823e-03\n",
            "Adam Iteration 15600: Loss = 1.54716e-03\n",
            "Adam Iteration 15700: Loss = 1.11023e-03\n",
            "Adam Iteration 15800: Loss = 2.14846e-03\n",
            "Adam Iteration 15900: Loss = 1.21619e-03\n",
            "Adam Iteration 16000: Loss = 1.90048e-03\n",
            "Adam Iteration 16100: Loss = 1.06409e-03\n",
            "Adam Iteration 16200: Loss = 1.36667e-03\n",
            "Adam Iteration 16300: Loss = 1.57560e-03\n",
            "Adam Iteration 16400: Loss = 1.09494e-03\n",
            "Adam Iteration 16500: Loss = 1.04168e-03\n",
            "Adam Iteration 16600: Loss = 9.92180e-04\n",
            "Adam Iteration 16700: Loss = 9.88580e-04\n",
            "Adam Iteration 16800: Loss = 1.37424e-03\n",
            "Adam Iteration 16900: Loss = 9.57402e-04\n",
            "Adam Iteration 17000: Loss = 9.63981e-04\n",
            "Adam Iteration 17100: Loss = 9.35451e-04\n",
            "Adam Iteration 17200: Loss = 9.68454e-04\n",
            "Adam Iteration 17300: Loss = 1.16654e-03\n",
            "Adam Iteration 17400: Loss = 9.93851e-04\n",
            "Adam Iteration 17500: Loss = 3.04262e-03\n",
            "Adam Iteration 17600: Loss = 8.82247e-04\n",
            "Adam Iteration 17700: Loss = 1.45923e-03\n",
            "Adam Iteration 17800: Loss = 9.33565e-04\n",
            "Adam Iteration 17900: Loss = 1.11296e-03\n",
            "Adam Iteration 18000: Loss = 5.20987e-03\n",
            "Adam Iteration 18100: Loss = 1.87831e-03\n",
            "Adam Iteration 18200: Loss = 1.10408e-03\n",
            "Adam Iteration 18300: Loss = 9.13983e-04\n",
            "Adam Iteration 18400: Loss = 8.73963e-04\n",
            "Adam Iteration 18500: Loss = 1.67688e-03\n",
            "Adam Iteration 18600: Loss = 8.28343e-04\n",
            "Adam Iteration 18700: Loss = 9.44818e-04\n",
            "Adam Iteration 18800: Loss = 9.74254e-04\n",
            "Adam Iteration 18900: Loss = 2.25187e-03\n",
            "Adam Iteration 19000: Loss = 7.47728e-04\n",
            "Adam Iteration 19100: Loss = 1.51532e-03\n",
            "Adam Iteration 19200: Loss = 8.42272e-04\n",
            "Adam Iteration 19300: Loss = 9.89097e-04\n",
            "Adam Iteration 19400: Loss = 7.65000e-04\n",
            "Adam Iteration 19500: Loss = 7.65357e-04\n",
            "Adam Iteration 19600: Loss = 6.92283e-04\n",
            "Adam Iteration 19700: Loss = 6.87951e-04\n",
            "Adam Iteration 19800: Loss = 8.34873e-04\n",
            "Adam Iteration 19900: Loss = 7.02119e-04\n",
            "L-BFGS Iteration 0: Loss = 6.51798e-04\n",
            "L-BFGS Iteration 100: Loss = 5.50793e-05\n",
            "L-BFGS Iteration 200: Loss = 1.88337e-05\n",
            "L-BFGS Iteration 300: Loss = 1.01166e-05\n",
            "L-BFGS Iteration 400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 1000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 1100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 1200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 1300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 1400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 1500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 1600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 1700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 1800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 1900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 2000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 2100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 2200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 2300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 2400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 2500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 2600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 2700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 2800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 2900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 3000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 3100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 3200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 3300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 3400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 3500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 3600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 3700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 3800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 3900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 4000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 4100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 4200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 4300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 4400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 4500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 4600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 4700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 4800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 4900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 5000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 5100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 5200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 5300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 5400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 5500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 5600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 5700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 5800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 5900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 6000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 6100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 6200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 6300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 6400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 6500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 6600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 6700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 6800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 6900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 7000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 7100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 7200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 7300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 7400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 7500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 7600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 7700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 7800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 7900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 8000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 8100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 8200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 8300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 8400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 8500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 8600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 8700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 8800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 8900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 9000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 9100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 9200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 9300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 9400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 9500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 9600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 9700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 9800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 9900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 10000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 10100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 10200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 10300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 10400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 10500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 10600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 10700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 10800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 10900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 11000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 11100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 11200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 11300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 11400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 11500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 11600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 11700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 11800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 11900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 12000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 12100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 12200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 12300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 12400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 12500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 12600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 12700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 12800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 12900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 13000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 13100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 13200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 13300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 13400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 13500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 13600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 13700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 13800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 13900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 14000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 14100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 14200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 14300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 14400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 14500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 14600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 14700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 14800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 14900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 15000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 15100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 15200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 15300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 15400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 15500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 15600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 15700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 15800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 15900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 16000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 16100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 16200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 16300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 16400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 16500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 16600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 16700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 16800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 16900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 17000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 17100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 17200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 17300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 17400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 17500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 17600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 17700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 17800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 17900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 18000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 18100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 18200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 18300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 18400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 18500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 18600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 18700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 18800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 18900: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 19000: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 19100: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 19200: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 19300: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 19400: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 19500: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 19600: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 19700: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 19800: Loss = 8.26007e-06\n",
            "L-BFGS Iteration 19900: Loss = 8.26007e-06\n",
            "Training time: 2565.4716\n",
            "Model saved to 'pytorch_model.pth'\n",
            "Error u: 1.978962e-02\n",
            "Error v: 2.402215e-02\n",
            "Error h: 1.046267e-02\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('pdf') \n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from pyDOE import lhs\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import json\n",
        "\n",
        "\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class PhysicsInformedNN(nn.Module):\n",
        "    def __init__(self, layers, lb, ub):\n",
        "        super(PhysicsInformedNN, self).__init__()\n",
        "        self.layers = layers\n",
        "        self.lb = torch.tensor(lb).float().to(device)\n",
        "        self.ub = torch.tensor(ub).float().to(device)\n",
        "\n",
        "        self.net = nn.Sequential()\n",
        "        for i in range(len(layers) - 1):\n",
        "            self.net.add_module(f\"linear_{i}\", nn.Linear(layers[i], layers[i+1]))\n",
        "            if i < len(layers) - 2:\n",
        "                self.net.add_module(f\"tanh_{i}\", nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def net_uv(self, x, t):\n",
        "        X = torch.cat([x, t], dim=1)\n",
        "        X = 2.0 * (X - self.lb) / (self.ub - self.lb) - 1.0\n",
        "        uv = self.forward(X)\n",
        "        u = uv[:, 0:1]\n",
        "        v = uv[:, 1:2]\n",
        "\n",
        "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True, allow_unused=True)[0]\n",
        "        v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True, allow_unused=True)[0]\n",
        "\n",
        "        return u, v, u_x, v_x\n",
        "\n",
        "    def net_f_uv(self, x, t):\n",
        "        u, v, u_x, v_x = self.net_uv(x, t)\n",
        "\n",
        "        u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True, allow_unused=True)[0]\n",
        "        u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True, allow_unused=True)[0]\n",
        "\n",
        "        v_t = torch.autograd.grad(v, t, grad_outputs=torch.ones_like(v), create_graph=True, allow_unused=True)[0]\n",
        "        v_xx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(v_x), create_graph=True, allow_unused=True)[0]\n",
        "\n",
        "        f_u = u_t + 0.5 * v_xx + (u**2 + v**2) * v\n",
        "        f_v = v_t - 0.5 * u_xx - (u**2 + u**2) * u # This line seems to have a typo, should be v**2\n",
        "        f_v = v_t - 0.5 * u_xx - (u**2 + v**2) * u\n",
        "\n",
        "\n",
        "        return f_u, f_v\n",
        "\n",
        "def train(model, optimizer_adam, optimizer_lbfgs, x0, t0, u0, v0, x_lb, t_lb, x_ub, t_ub, x_f, t_f, niter_adam=20000):\n",
        "    model = model.to(device)\n",
        "\n",
        "    x0 = torch.tensor(x0).float().to(device).requires_grad_(True)\n",
        "    t0 = torch.tensor(t0).float().to(device).requires_grad_(True)\n",
        "    u0 = torch.tensor(u0).float().to(device)\n",
        "    v0 = torch.tensor(v0).float().to(device)\n",
        "\n",
        "    x_lb = torch.tensor(x_lb).float().to(device).requires_grad_(True)\n",
        "    t_lb = torch.tensor(t_lb).float().to(device).requires_grad_(True)\n",
        "\n",
        "    x_ub = torch.tensor(x_ub).float().to(device).requires_grad_(True)\n",
        "    t_ub = torch.tensor(t_ub).float().to(device).requires_grad_(True)\n",
        "\n",
        "    x_f = torch.tensor(x_f).float().to(device).requires_grad_(True)\n",
        "    t_f = torch.tensor(t_f).float().to(device).requires_grad_(True)\n",
        "\n",
        "    def compute_loss():\n",
        "        u0_pred, v0_pred, _, _ = model.net_uv(x0, t0)\n",
        "        u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred = model.net_uv(x_lb, t_lb)\n",
        "        u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred = model.net_uv(x_ub, t_ub)\n",
        "        f_u_pred, f_v_pred = model.net_f_uv(x_f, t_f)\n",
        "\n",
        "        loss = torch.mean((u0 - u0_pred)**2) + \\\n",
        "               torch.mean((v0 - v0_pred)**2) + \\\n",
        "               torch.mean((u_lb_pred - u_ub_pred)**2) + \\\n",
        "               torch.mean((v_lb_pred - v_ub_pred)**2) + \\\n",
        "               torch.mean((u_x_lb_pred - u_x_ub_pred)**2) + \\\n",
        "               torch.mean((v_x_lb_pred - v_x_ub_pred)**2) + \\\n",
        "               torch.mean(f_u_pred**2) + \\\n",
        "               torch.mean(f_v_pred**2)\n",
        "        return loss\n",
        "\n",
        "   \n",
        "    for it in range(niter_adam):\n",
        "        optimizer_adam.zero_grad()\n",
        "        loss = compute_loss()\n",
        "        loss.backward()\n",
        "        optimizer_adam.step()\n",
        "        if it % 100 == 0:\n",
        "            print(f'Adam Iteration {it}: Loss = {loss.item():.5e}')\n",
        "\n",
        "   \n",
        "    def closure():\n",
        "        optimizer_lbfgs.zero_grad()\n",
        "        loss = compute_loss()\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    for it in range(20000):  # Max iterations for L-BFGS\n",
        "        optimizer_lbfgs.step(closure)\n",
        "        loss = closure()\n",
        "        if it % 100 == 0:\n",
        "            print(f'L-BFGS Iteration {it}: Loss = {loss.item():.5e}')\n",
        "\n",
        "def predict(model, X_star, batch_size=4096):\n",
        "    model.eval()\n",
        "    n_points = X_star.shape[0]\n",
        "    u_pred = np.zeros((n_points, 1))\n",
        "    v_pred = np.zeros((n_points, 1))\n",
        "    f_u_pred = np.zeros((n_points, 1))\n",
        "    f_v_pred = np.zeros((n_points, 1))\n",
        "\n",
        "    for i in range(0, n_points, batch_size):\n",
        "        x_batch = torch.tensor(X_star[i:i+batch_size, 0:1]).float().to(device).requires_grad_(True)\n",
        "        t_batch = torch.tensor(X_star[i:i+batch_size, 1:2]).float().to(device).requires_grad_(True)\n",
        "\n",
        "        \n",
        "        u_batch, v_batch, u_x_batch, v_x_batch = model.net_uv(x_batch, t_batch)\n",
        "        u_t_batch = torch.autograd.grad(u_batch, t_batch, grad_outputs=torch.ones_like(u_batch), create_graph=True, allow_unused=True)[0]\n",
        "        u_xx_batch = torch.autograd.grad(u_x_batch, x_batch, grad_outputs=torch.ones_like(u_x_batch), create_graph=True, allow_unused=True)[0]\n",
        "        v_t_batch = torch.autograd.grad(v_batch, t_batch, grad_outputs=torch.ones_like(v_batch), create_graph=True, allow_unused=True)[0]\n",
        "        v_xx_batch = torch.autograd.grad(v_x_batch, x_batch, grad_outputs=torch.ones_like(v_x_batch), create_graph=True, allow_unused=True)[0]\n",
        "\n",
        "        \n",
        "        f_u_batch = u_t_batch + 0.5 * v_xx_batch + (u_batch**2 + v_batch**2) * v_batch\n",
        "        f_v_batch = v_t_batch - 0.5 * u_xx_batch - (u_batch**2 + v_batch**2) * u_batch\n",
        "\n",
        "        \n",
        "        u_pred[i:i+batch_size, :] = u_batch.cpu().detach().numpy()\n",
        "        v_pred[i:i+batch_size, :] = v_batch.cpu().detach().numpy()\n",
        "        f_u_pred[i:i+batch_size, :] = f_u_batch.cpu().detach().numpy()\n",
        "        f_v_pred[i:i+batch_size, :] = f_v_batch.cpu().detach().numpy()\n",
        "\n",
        "    return u_pred, v_pred, f_u_pred, f_v_pred\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    noise = 0.0\n",
        "\n",
        "    \n",
        "    lb = np.array([0.0, 0.0])\n",
        "    ub = np.array([20.0, 5.0])\n",
        "\n",
        "    N0 = 100\n",
        "    N_b = 100\n",
        "    N_f = 20000\n",
        "    layers = [2, 80, 80, 80, 80, 2]\n",
        "\n",
        "    data = scipy.io.loadmat('NLS_2soliton_interaction.mat')\n",
        "\n",
        "    t = data['t_values'].flatten()[:, None]\n",
        "    x = data['x_values'].flatten()[:, None]\n",
        "    Exact = data['q_values']\n",
        "\n",
        "    Exact_u = np.real(Exact)\n",
        "    Exact_v = np.imag(Exact)\n",
        "    Exact_h = np.sqrt(Exact_u ** 2 + Exact_v ** 2)\n",
        "\n",
        "    X, T = np.meshgrid(x, t)\n",
        "\n",
        "    X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
        "    u_star = Exact_u.T.flatten()[:, None]\n",
        "    v_star = Exact_v.T.flatten()[:, None]\n",
        "    h_star = Exact_h.T.flatten()[:, None]\n",
        "\n",
        "    idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
        "    x0 = x[idx_x, :]\n",
        "    u0 = Exact_u[idx_x, 0:1]\n",
        "    v0 = Exact_v[idx_x, 0:1]\n",
        "\n",
        "    idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
        "    tb = t[idx_t, :]\n",
        "\n",
        "    X_f = lb + (ub - lb) * lhs(2, N_f)\n",
        "\n",
        "    X0 = np.concatenate((x0, 0 * x0), 1)  # (x0, 0)\n",
        "    X_lb = np.concatenate((0 * tb + lb[0], tb), 1)  # (lb[0], tb)\n",
        "    X_ub = np.concatenate((0 * tb + ub[0], tb), 1)  # (ub[0], tb)\n",
        "    X_u_train = np.vstack([X0, X_lb, X_ub])\n",
        "\n",
        "    model = PhysicsInformedNN(layers, lb, ub)\n",
        "    optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n",
        "    optimizer_lbfgs = optim.LBFGS(model.parameters(), max_iter=20, history_size=50, line_search_fn=\"strong_wolfe\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    train(model, optimizer_adam, optimizer_lbfgs,\n",
        "          X0[:, 0:1], X0[:, 1:2], u0, v0,\n",
        "          X_lb[:, 0:1], X_lb[:, 1:2],\n",
        "          X_ub[:, 0:1], X_ub[:, 1:2],\n",
        "          X_f[:, 0:1], X_f[:, 1:2])\n",
        "    elapsed = time.time() - start_time\n",
        "    print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "    \n",
        "    torch.save(model.state_dict(), 'pytorch_model.pth')\n",
        "    print(\"Model saved to 'pytorch_model.pth'\")\n",
        "\n",
        "    u_pred, v_pred, f_u_pred, f_v_pred = predict(model, X_star)\n",
        "    h_pred = np.sqrt(u_pred ** 2 + v_pred ** 2)\n",
        "\n",
        "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "    error_v = np.linalg.norm(v_star - v_pred, 2) / np.linalg.norm(v_star, 2)\n",
        "    error_h = np.linalg.norm(h_star - h_pred, 2) / np.linalg.norm(h_star, 2)\n",
        "    print('Error u: %e' % (error_u))\n",
        "    print('Error v: %e' % (error_v))\n",
        "    print('Error h: %e' % (error_h))\n",
        "\n",
        "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
        "    V_pred = griddata(X_star, v_pred.flatten(), (X, T), method='cubic')\n",
        "    H_pred = griddata(X_star, h_pred.flatten(), (X, T), method='cubic')\n",
        "\n",
        "    U_star = griddata(X_star, u_star.flatten(), (X, T), method='cubic')\n",
        "    V_star = griddata(X_star, v_star.flatten(), (X, T), method='cubic')\n",
        "    H_star = griddata(X_star, h_star.flatten(), (X, T), method='cubic')\n",
        "\n",
        "    FU_pred = griddata(X_star, f_u_pred.flatten(), (X, T), method='cubic')\n",
        "    FV_pred = griddata(X_star, f_v_pred.flatten(), (X, T), method='cubic')\n",
        "\n",
        "    ########################################################################\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    x_min, x_max = x.min(), x.max()\n",
        "    cut_indices = np.linspace(0, len(t) - 1, 9, dtype=int)\n",
        "    y_min, y_max = -0.1, (len(cut_indices) + 1) * 2.0\n",
        "\n",
        "    for i, idx in enumerate(cut_indices):\n",
        "        vertical_offset = i * 2.0  # Adjust the spacing between lines as needed\n",
        "        plt.plot(x, Exact_h[:, idx] + vertical_offset, 'b-', linewidth=1.5, label=f'$t = {t[idx, 0]:.2f}$' if i == 0 else \"\")\n",
        "        plt.plot(x, H_pred[idx, :] + vertical_offset, 'r--', linewidth=1.5)\n",
        "        plt.text(x_min - 1, vertical_offset, f'$t = {t[idx, 0]:.2f}$', fontsize=10, verticalalignment='center')\n",
        "    plt.xlabel('$x$', fontsize=14)\n",
        "    plt.ylabel('')\n",
        "    plt.xlim([x_min, x_max])\n",
        "    plt.ylim([y_min, y_max])\n",
        "    plt.gca().set_yticks([])  # Remove y-axis ticks\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "    plt.legend(['Exact', 'Prediction'], loc='upper right', fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('time.pdf', dpi=300)\n",
        "\n",
        "    ############################ Plotting ###############################\n",
        "   \n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    \n",
        "    h_img = ax.imshow(H_pred,\n",
        "                      extent=[lb[0], ub[0], lb[1], ub[1]],  # [x_min, x_max, t_min, t_max]\n",
        "                      origin='lower',\n",
        "                      aspect='auto',\n",
        "                      cmap='YlGnBu')\n",
        "\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    fig.colorbar(h_img, cax=cax, label='|Q(x, t)|')\n",
        "\n",
        "    \n",
        "    ax.plot(X_u_train[:, 0], X_u_train[:, 1],\n",
        "            'kx', label='Data Points (%d)' % (X_u_train.shape[0]),\n",
        "            markersize=4, clip_on=False)\n",
        "    ax.plot(X_f[:, 0], X_f[:, 1],\n",
        "            'r.', label='Collocation Points (%d)' % (X_f.shape[0]),\n",
        "            markersize=2)\n",
        "    ax.legend(loc='upper right', fontsize=10, framealpha=0.8)\n",
        "    ax.set_xlabel(\"x\", fontsize=14)\n",
        "    ax.set_ylabel(\"t\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('data.pdf', dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    #############################################################################\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Plotting exact u(t, x)\n",
        "    plt.subplot(3, 3, 1)\n",
        "    plt.pcolor(T, X, U_star, cmap='gray')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('$t$')\n",
        "    plt.ylabel('$x$')\n",
        "    plt.title('Exact u(t, x)')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Plotting exact v(t, x)\n",
        "    plt.subplot(3, 3, 2)\n",
        "    plt.pcolor(T, X, V_star, cmap='gray')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('$t$')\n",
        "    plt.ylabel('$x$')\n",
        "    plt.title('Exact v(t, x)')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Plotting exact h(t, x)\n",
        "    plt.subplot(3, 3, 3)\n",
        "    plt.pcolor(T, X, H_star, cmap='gray')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('$t$')\n",
        "    plt.ylabel('$x$')\n",
        "    plt.title('Exact h(t, x)')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Plotting predicted u(t, x)\n",
        "    plt.subplot(3, 3, 4)\n",
        "    plt.pcolor(T, X, U_pred, cmap='gray')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('$t$')\n",
        "    plt.ylabel('$x$')\n",
        "    plt.title('Predicted u(t, x)')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Plotting predicted v(t, x)\n",
        "    plt.subplot(3, 3, 5)\n",
        "    plt.pcolor(T, X, V_pred, cmap='gray')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('$t$')\n",
        "    plt.ylabel('$x$')\n",
        "    plt.title('Predicted v(t, x)')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Plotting predicted h(t, x)\n",
        "    plt.subplot(3, 3, 6)\n",
        "    plt.pcolor(T, X, H_pred, cmap='gray')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('$t$')\n",
        "    plt.ylabel('$x$')\n",
        "    plt.title('Predicted h(t, x)')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Plotting absolute error for u(t, x)\n",
        "    plt.subplot(3, 3, 7)\n",
        "    plt.pcolor(T, X, np.abs(U_star - U_pred), cmap='gray')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('$t$')\n",
        "    plt.ylabel('$x$')\n",
        "    plt.title('Absolute error for u(t, x)')\n",
        "    plt.text(0.1, 0.9, f'Error u: {error_u:.3e}', color='white', fontsize=10, transform=plt.gca().transAxes)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Plotting absolute error for v(t, x)\n",
        "    plt.subplot(3, 3, 8)\n",
        "    plt.pcolor(T, X, np.abs(V_star - V_pred), cmap='gray')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('$t$')\n",
        "    plt.ylabel('$x$')\n",
        "    plt.title('Absolute error for v(t, x)')\n",
        "    plt.text(0.1, 0.9, f'Error v: {error_v:.3e}', color='white', fontsize=10, transform=plt.gca().transAxes)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Plotting absolute error for h(t, x)\n",
        "    plt.subplot(3, 3, 9)\n",
        "    plt.pcolor(T, X, np.abs(H_star - H_pred), cmap='gray')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('$t$')\n",
        "    plt.ylabel('$x$')\n",
        "    plt.title('Absolute error for h(t, x)')\n",
        "    plt.text(0.1, 0.9, f'Error h: {error_h:.3e}', color='white', fontsize=10, transform=plt.gca().transAxes)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('U_V_H_errors.pdf', dpi=25, bbox_inches='tight')\n",
        "\n",
        "    np.savez_compressed('PINN_results.npz',\n",
        "        x=X[0, :],                  # x-axis\n",
        "        t=T[:, 0],                  # t-axis\n",
        "        X=X,\n",
        "        T=T,\n",
        "        Exact_h = Exact_h,\n",
        "        U_star=U_star,\n",
        "        V_star=V_star,\n",
        "        H_star=H_star,\n",
        "        U_pred=U_pred,\n",
        "        V_pred=V_pred,\n",
        "        H_pred=H_pred,\n",
        "        X_u_train=X_u_train,\n",
        "        X_f=X_f,\n",
        "        lb=lb,\n",
        "        ub=ub,\n",
        "        error_u=error_u,\n",
        "        error_v=error_v,\n",
        "        error_h=error_h\n",
        "    )\n",
        "\n",
        "    metadata = {\n",
        "        'layers': layers,\n",
        "        'N0': int(N0),\n",
        "        'Nb': int(N_b),\n",
        "        'Nf': int(N_f),\n",
        "        'lb': lb.tolist(),\n",
        "        'ub': ub.tolist(),\n",
        "        'error_u': float(error_u),\n",
        "        'error_v': float(error_v),\n",
        "        'error_h': float(error_h),\n",
        "        'training_time_sec': float(elapsed)\n",
        "    }\n",
        "\n",
        "    with open('PINN_metadata.json', 'w') as f:\n",
        "        json.dump(metadata, f, indent=4)\n",
        "\n",
        "    \n",
        "    fig_anim, ax_anim = plt.subplots(figsize=(10, 6))\n",
        "    x_flat = x.flatten()  # Ensure x is 1D\n",
        "    t_flat = t.flatten()  # Ensure t is 1D\n",
        "\n",
        "    \n",
        "    line_exact, = ax_anim.plot(x_flat, H_star[0, :], 'b-', label='Exact', lw=2)\n",
        "    line_pred, = ax_anim.plot(x_flat, H_pred[0, :], 'r--', label='PINN', lw=2)\n",
        "\n",
        "   \n",
        "    ax_anim.set_xlim(x_flat.min(), x_flat.max())\n",
        "    ax_anim.set_ylim(-0.5, H_star.max() + 0.5)  # Adjust ylim for magnitude (non-negative)\n",
        "    ax_anim.set_title('Evolucin |q(x,t)|')\n",
        "    ax_anim.set_xlabel('x')\n",
        "    ax_anim.set_ylabel('|q|')\n",
        "    ax_anim.legend(loc='upper right')\n",
        "    ax_anim.grid(True)\n",
        "\n",
        "    def animate(i):\n",
        "        line_exact.set_ydata(H_star[i, :])\n",
        "        line_pred.set_ydata(H_pred[i, :])\n",
        "        ax_anim.set_title(f'Evolucin |q(x,t={t_flat[i]:.2f})')\n",
        "        return line_exact, line_pred\n",
        "\n",
        "    anim = FuncAnimation(fig_anim, animate, frames=len(t_flat), interval=100, blit=False, repeat=True)\n",
        "    anim.save('nls_evolution.gif', writer='pillow', fps=10)\n",
        "    plt.show()  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
